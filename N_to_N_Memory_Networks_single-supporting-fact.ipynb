{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdR0pl4HSdX6"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1m9MfQaUS7rv"
   },
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path) as f:\n",
    "        file = f.readlines()\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDcK-yoBTEnl"
   },
   "outputs": [],
   "source": [
    "train_unp = read_file('/content/qa1_single-supporting-fact_train.txt')\n",
    "test_unp  = read_file('/content/qa1_single-supporting-fact_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qfm8Ac4OTYxU",
    "outputId": "235d3df4-92b9-4e26-b1b4-ef37a3b60f85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_unp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "J9QFukdOTbiu",
    "outputId": "b3212eb5-e06c-4905-e297-614c6fd3b316"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 Mary moved to the bathroom.\\n',\n",
       " '2 John went to the hallway.\\n',\n",
       " '3 Where is Mary? \\tbathroom\\t1\\n',\n",
       " '4 Daniel went back to the hallway.\\n',\n",
       " '5 Sandra moved to the garden.\\n',\n",
       " '6 Where is Daniel? \\thallway\\t4\\n',\n",
       " '7 John moved to the office.\\n',\n",
       " '8 Sandra journeyed to the bathroom.\\n',\n",
       " '9 Where is Daniel? \\thallway\\t4\\n',\n",
       " '10 Mary moved to the hallway.\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unp[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRdtVst1Te5q"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(data):\n",
    "    \n",
    "    ret = []\n",
    "    l1 = []\n",
    "    que = []\n",
    "    \n",
    "    for i in range (len(data)):\n",
    "        if (i+1)%3 != 0:\n",
    "            for word in data[i].split():\n",
    "                if re.match(r'\\d',word) == None:\n",
    "#                 if word not in '0123456789':\n",
    "                    if word[-1] in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~':\n",
    "                        l1.append(word[0:-1])\n",
    "                        l1.append(word[-1])\n",
    "                    else:\n",
    "                        l1.append(word)\n",
    "        else:\n",
    "            temp = data[i].split('\\t')\n",
    "            ans = temp[1]\n",
    "            for word in temp[0].split():\n",
    "                if re.match(r'\\d',word) == None:\n",
    "#                 if word not in '0123456789':\n",
    "                    if word[-1] in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~':\n",
    "                        que.append(word[0:-1])\n",
    "                        que.append(word[-1])\n",
    "                    else:\n",
    "                        que.append(word)\n",
    "\n",
    "            tup = (l1,que,ans)\n",
    "            ret.append(tup)\n",
    "            l1 = []\n",
    "            que = []\n",
    "            \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wty-FktETjxm"
   },
   "outputs": [],
   "source": [
    "train = pre_process(train_unp)\n",
    "test = pre_process(test_unp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "i0P_VpENTnN3",
    "outputId": "258b26af-12ba-43c3-bb3b-685075a8f834"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Mary',\n",
       "   'moved',\n",
       "   'to',\n",
       "   'the',\n",
       "   'bathroom',\n",
       "   '.',\n",
       "   'John',\n",
       "   'went',\n",
       "   'to',\n",
       "   'the',\n",
       "   'hallway',\n",
       "   '.'],\n",
       "  ['Where', 'is', 'Mary', '?'],\n",
       "  'bathroom'),\n",
       " (['Daniel',\n",
       "   'went',\n",
       "   'back',\n",
       "   'to',\n",
       "   'the',\n",
       "   'hallway',\n",
       "   '.',\n",
       "   'Sandra',\n",
       "   'moved',\n",
       "   'to',\n",
       "   'the',\n",
       "   'garden',\n",
       "   '.'],\n",
       "  ['Where', 'is', 'Daniel', '?'],\n",
       "  'hallway')]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_m4y4vJETphr"
   },
   "outputs": [],
   "source": [
    "all_data = train + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gavk8IDaTzYU"
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for story, query, ans in all_data:\n",
    "    \n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(query))\n",
    "    if ans not in vocab:\n",
    "        vocab.add(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eT6oYh7kT10k",
    "outputId": "e906478e-7473-4589-b889-ea4968dee62f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "R2-IQnm1T4Mu",
    "outputId": "b4926a4f-d384-4abb-ddb2-bfff94bee8a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'Where',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'garden',\n",
       " 'hallway',\n",
       " 'is',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'moved',\n",
       " 'office',\n",
       " 'the',\n",
       " 'to',\n",
       " 'travelled',\n",
       " 'went'}"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vV-itt7DT6fi",
    "outputId": "9e917944-1b1d-4eeb-df77-31ae6c278ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len = len(vocab) + 1\n",
    "max_story_len =max([len(data[0]) for data in all_data])\n",
    "max_story_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5icIbuX0VFiV",
    "outputId": "314813ce-2641-4154-d95e-a816bd83db1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len = max([len(data[1]) for data in all_data])\n",
    "max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ipQBiW4VVTjU",
    "outputId": "c4354e70-2f98-474b-bd2a-d5243dfa9158"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSC0rvY5WJFV"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "nd4ElpYGWThv",
    "outputId": "51336d4a-1453-49a3-e5a1-e2964b02aa37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 5,\n",
       " '?': 19,\n",
       " 'back': 7,\n",
       " 'bathroom': 4,\n",
       " 'bedroom': 11,\n",
       " 'daniel': 8,\n",
       " 'garden': 9,\n",
       " 'hallway': 6,\n",
       " 'is': 2,\n",
       " 'john': 10,\n",
       " 'journeyed': 21,\n",
       " 'kitchen': 18,\n",
       " 'mary': 15,\n",
       " 'moved': 1,\n",
       " 'office': 14,\n",
       " 'sandra': 17,\n",
       " 'the': 12,\n",
       " 'to': 16,\n",
       " 'travelled': 13,\n",
       " 'went': 20,\n",
       " 'where': 3}"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "szSEb1V-Wgwp",
    "outputId": "55757096-1ac5-458f-9514-ab407756b005"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15], [1], [16], [12], [4], [5], [10], [20], [16], [12], [6], [5]]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikvzaILfXvVe"
   },
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):\n",
    "    '''\n",
    "    INPUT: \n",
    "    \n",
    "    data: consisting of Stories,Queries,and Answers\n",
    "    word_index: word index dictionary from tokenizer\n",
    "    max_story_len: the length of the longest story (used for pad_sequences function)\n",
    "    max_question_len: length of the longest question (used for pad_sequences function)\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "    \n",
    "    Vectorizes the stories,questions, and answers into padded sequences. We first loop for every story, query , and\n",
    "    answer in the data. Then we convert the raw words to an word index value. Then we append each set to their appropriate\n",
    "    output list. Then once we have converted the words to numbers, we pad the sequences so they are all of equal length.\n",
    "    \n",
    "    Returns this in the form of a tuple (X,Xq,Y) (padded based on max lengths)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # X = STORIES\n",
    "    X = []\n",
    "    # Xq = QUERY/QUESTION\n",
    "    Xq = []\n",
    "    # Y = CORRECT ANSWER\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        \n",
    "        # Grab the word index for every word in story\n",
    "        x = [word_index[word.lower()] for word in story]\n",
    "        # Grab the word index for every word in query\n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        \n",
    "        # Grab the Answers (either Yes/No so we don't need to use list comprehension here)\n",
    "        # Index 0 is reserved so we're going to use + 1\n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        \n",
    "        # Now that y is all zeros and we know its just Yes/No , we can use numpy logic to create this assignment\n",
    "        #\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        # Append each set of story,query, and answer to their respective holding lists\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        \n",
    "    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.\n",
    "        \n",
    "    # RETURN TUPLE FOR UNPACKING\n",
    "    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pbFJBPNVbfgH"
   },
   "outputs": [],
   "source": [
    "story_train, queries_train, answers_train = vectorize_stories(train)\n",
    "story_test, queries_test, answers_test = vectorize_stories(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pfgPtSCbxkX"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM,Dense,Embedding,Input,Permute,Activation,Dropout\n",
    "from keras.layers import add,dot,concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frEdSATbbz6N"
   },
   "outputs": [],
   "source": [
    "input_seq = Input((max_story_len,))\n",
    "question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "BqH5Bu6XdH0I",
    "outputId": "ea92a6c6-a76e-43b5-d099-bc5dd1f5b294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim = vocab_len,output_dim=64))\n",
    "input_encoder_m.add(Dropout(rate = 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUXtPNfmen0S"
   },
   "outputs": [],
   "source": [
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim = vocab_len,output_dim=max_question_len))\n",
    "input_encoder_c.add(Dropout(rate = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fENkLIdVetN-"
   },
   "outputs": [],
   "source": [
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim = vocab_len,output_dim=64, input_length=max_question_len))\n",
    "question_encoder.add(Dropout(rate = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQRUM6CNhOXN"
   },
   "outputs": [],
   "source": [
    "input_encoded_m = input_encoder_m(input_seq)\n",
    "input_encoded_c = input_encoder_c(input_seq)\n",
    "question_encoded = question_encoder(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "co4MgLhNhn0T"
   },
   "outputs": [],
   "source": [
    "match = dot([input_encoded_m,question_encoded],axes=(2,2))\n",
    "match = Activation('softmax')(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_R645gNiQpg"
   },
   "outputs": [],
   "source": [
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z8Ae4JrDiXFH",
    "outputId": "74379627-8cfd-46e4-a01b-b096974107be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_1/concat:0' shape=(?, 4, 78) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZv7rDFSicJR"
   },
   "outputs": [],
   "source": [
    "answer = LSTM(32)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zYbt43T4ioxX"
   },
   "outputs": [],
   "source": [
    "answer = Dropout(rate = 0.5)(answer)\n",
    "answer = Dense(vocab_len)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoz-LJOsiwnE"
   },
   "outputs": [],
   "source": [
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "model = Model([input_seq, question],answer)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "xDu5YSqKjhZa",
    "outputId": "1ba6ace8-bf17-45c7-f43a-cb81b92ad8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             1408        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 4, 64)        1408        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 14, 4)        0           sequential_1[1][0]               \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 14, 4)        0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       multiple             88          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 14, 4)        0           activation_1[0][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 4, 14)        0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 78)        0           permute_1[0][0]                  \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           14208       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 22)           726         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 22)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,838\n",
      "Trainable params: 17,838\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17156
    },
    "colab_type": "code",
    "id": "7h74rIZljlBT",
    "outputId": "843b878d-adae-432f-88df-5a362469cd24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/500\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.7507 - acc: 0.1540 - val_loss: 2.2199 - val_acc: 0.1540\n",
      "Epoch 2/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 2.1611 - acc: 0.1560 - val_loss: 1.9150 - val_acc: 0.1540\n",
      "Epoch 3/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 2.0210 - acc: 0.1770 - val_loss: 1.8499 - val_acc: 0.1540\n",
      "Epoch 4/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.9629 - acc: 0.1820 - val_loss: 1.8400 - val_acc: 0.1570\n",
      "Epoch 5/500\n",
      "1000/1000 [==============================] - 1s 653us/step - loss: 1.9383 - acc: 0.1610 - val_loss: 1.8290 - val_acc: 0.1570\n",
      "Epoch 6/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.9192 - acc: 0.1600 - val_loss: 1.8165 - val_acc: 0.1890\n",
      "Epoch 7/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 1.9085 - acc: 0.1460 - val_loss: 1.8179 - val_acc: 0.1670\n",
      "Epoch 8/500\n",
      "1000/1000 [==============================] - 1s 664us/step - loss: 1.8746 - acc: 0.1630 - val_loss: 1.8044 - val_acc: 0.1870\n",
      "Epoch 9/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 1.8629 - acc: 0.1730 - val_loss: 1.8037 - val_acc: 0.1760\n",
      "Epoch 10/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.8631 - acc: 0.1820 - val_loss: 1.7961 - val_acc: 0.1870\n",
      "Epoch 11/500\n",
      "1000/1000 [==============================] - 1s 658us/step - loss: 1.8537 - acc: 0.1710 - val_loss: 1.7914 - val_acc: 0.1870\n",
      "Epoch 12/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.8318 - acc: 0.1840 - val_loss: 1.7881 - val_acc: 0.2450\n",
      "Epoch 13/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.8091 - acc: 0.2010 - val_loss: 1.7817 - val_acc: 0.2530\n",
      "Epoch 14/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.8083 - acc: 0.1890 - val_loss: 1.7749 - val_acc: 0.2400\n",
      "Epoch 15/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.8045 - acc: 0.1970 - val_loss: 1.7664 - val_acc: 0.2190\n",
      "Epoch 16/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.8007 - acc: 0.1890 - val_loss: 1.7491 - val_acc: 0.2470\n",
      "Epoch 17/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.7565 - acc: 0.2650 - val_loss: 1.7266 - val_acc: 0.2520\n",
      "Epoch 18/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.7425 - acc: 0.2280 - val_loss: 1.7081 - val_acc: 0.2550\n",
      "Epoch 19/500\n",
      "1000/1000 [==============================] - 1s 668us/step - loss: 1.7202 - acc: 0.2510 - val_loss: 1.6845 - val_acc: 0.2660\n",
      "Epoch 20/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.7077 - acc: 0.2490 - val_loss: 1.6724 - val_acc: 0.2950\n",
      "Epoch 21/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.6906 - acc: 0.2740 - val_loss: 1.6736 - val_acc: 0.2750\n",
      "Epoch 22/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.6796 - acc: 0.2800 - val_loss: 1.6667 - val_acc: 0.2880\n",
      "Epoch 23/500\n",
      "1000/1000 [==============================] - 1s 662us/step - loss: 1.6870 - acc: 0.2580 - val_loss: 1.6643 - val_acc: 0.3720\n",
      "Epoch 24/500\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 1.6554 - acc: 0.3020 - val_loss: 1.6515 - val_acc: 0.3050\n",
      "Epoch 25/500\n",
      "1000/1000 [==============================] - 1s 665us/step - loss: 1.6784 - acc: 0.2970 - val_loss: 1.6560 - val_acc: 0.3200\n",
      "Epoch 26/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.6760 - acc: 0.2870 - val_loss: 1.6514 - val_acc: 0.3250\n",
      "Epoch 27/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.6702 - acc: 0.3060 - val_loss: 1.6492 - val_acc: 0.3440\n",
      "Epoch 28/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.6564 - acc: 0.2980 - val_loss: 1.6447 - val_acc: 0.3330\n",
      "Epoch 29/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.6326 - acc: 0.3100 - val_loss: 1.6593 - val_acc: 0.3550\n",
      "Epoch 30/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.6643 - acc: 0.3020 - val_loss: 1.6411 - val_acc: 0.3390\n",
      "Epoch 31/500\n",
      "1000/1000 [==============================] - 1s 669us/step - loss: 1.6340 - acc: 0.3260 - val_loss: 1.6412 - val_acc: 0.3450\n",
      "Epoch 32/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 1.6468 - acc: 0.3050 - val_loss: 1.6287 - val_acc: 0.3630\n",
      "Epoch 33/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.6327 - acc: 0.3370 - val_loss: 1.6290 - val_acc: 0.3880\n",
      "Epoch 34/500\n",
      "1000/1000 [==============================] - 1s 653us/step - loss: 1.6255 - acc: 0.3430 - val_loss: 1.6223 - val_acc: 0.3640\n",
      "Epoch 35/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.6158 - acc: 0.3290 - val_loss: 1.6230 - val_acc: 0.3880\n",
      "Epoch 36/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.6289 - acc: 0.3420 - val_loss: 1.6152 - val_acc: 0.3810\n",
      "Epoch 37/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 1.6093 - acc: 0.3600 - val_loss: 1.6044 - val_acc: 0.3840\n",
      "Epoch 38/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.6307 - acc: 0.3330 - val_loss: 1.5996 - val_acc: 0.3990\n",
      "Epoch 39/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.6116 - acc: 0.3610 - val_loss: 1.5947 - val_acc: 0.3880\n",
      "Epoch 40/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.5874 - acc: 0.3620 - val_loss: 1.5969 - val_acc: 0.3940\n",
      "Epoch 41/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.6092 - acc: 0.3530 - val_loss: 1.5905 - val_acc: 0.3940\n",
      "Epoch 42/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 1.5916 - acc: 0.3710 - val_loss: 1.5807 - val_acc: 0.4170\n",
      "Epoch 43/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 1.5895 - acc: 0.3620 - val_loss: 1.5743 - val_acc: 0.4110\n",
      "Epoch 44/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.5879 - acc: 0.3650 - val_loss: 1.5782 - val_acc: 0.3970\n",
      "Epoch 45/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 1.5873 - acc: 0.3660 - val_loss: 1.5710 - val_acc: 0.3940\n",
      "Epoch 46/500\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 1.5764 - acc: 0.3810 - val_loss: 1.5654 - val_acc: 0.4060\n",
      "Epoch 47/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.5696 - acc: 0.3750 - val_loss: 1.5624 - val_acc: 0.3990\n",
      "Epoch 48/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.5734 - acc: 0.3890 - val_loss: 1.5750 - val_acc: 0.3870\n",
      "Epoch 49/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.5567 - acc: 0.3920 - val_loss: 1.5637 - val_acc: 0.4010\n",
      "Epoch 50/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.5806 - acc: 0.3860 - val_loss: 1.5709 - val_acc: 0.3800\n",
      "Epoch 51/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.5615 - acc: 0.3750 - val_loss: 1.5568 - val_acc: 0.3970\n",
      "Epoch 52/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.5684 - acc: 0.3960 - val_loss: 1.5539 - val_acc: 0.4070\n",
      "Epoch 53/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.5735 - acc: 0.3900 - val_loss: 1.5548 - val_acc: 0.3890\n",
      "Epoch 54/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.5638 - acc: 0.4080 - val_loss: 1.5565 - val_acc: 0.3880\n",
      "Epoch 55/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 1.5486 - acc: 0.3990 - val_loss: 1.5492 - val_acc: 0.4140\n",
      "Epoch 56/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.5746 - acc: 0.3930 - val_loss: 1.5648 - val_acc: 0.3870\n",
      "Epoch 57/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.5435 - acc: 0.3890 - val_loss: 1.5513 - val_acc: 0.4090\n",
      "Epoch 58/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 1.5537 - acc: 0.4020 - val_loss: 1.5461 - val_acc: 0.4190\n",
      "Epoch 59/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.5185 - acc: 0.3930 - val_loss: 1.5532 - val_acc: 0.3990\n",
      "Epoch 60/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 1.5308 - acc: 0.3980 - val_loss: 1.5424 - val_acc: 0.4060\n",
      "Epoch 61/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.5624 - acc: 0.3980 - val_loss: 1.5457 - val_acc: 0.3940\n",
      "Epoch 62/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.5178 - acc: 0.4210 - val_loss: 1.5428 - val_acc: 0.3960\n",
      "Epoch 63/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.5538 - acc: 0.4100 - val_loss: 1.5401 - val_acc: 0.4190\n",
      "Epoch 64/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.5285 - acc: 0.4030 - val_loss: 1.5422 - val_acc: 0.4120\n",
      "Epoch 65/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.5101 - acc: 0.4160 - val_loss: 1.5433 - val_acc: 0.3970\n",
      "Epoch 66/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.5365 - acc: 0.4120 - val_loss: 1.5420 - val_acc: 0.4090\n",
      "Epoch 67/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 1.5375 - acc: 0.4130 - val_loss: 1.5538 - val_acc: 0.3960\n",
      "Epoch 68/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.5232 - acc: 0.4200 - val_loss: 1.5403 - val_acc: 0.4070\n",
      "Epoch 69/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.5218 - acc: 0.4250 - val_loss: 1.5425 - val_acc: 0.3900\n",
      "Epoch 70/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 1.5162 - acc: 0.4140 - val_loss: 1.5440 - val_acc: 0.4010\n",
      "Epoch 71/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.5298 - acc: 0.4150 - val_loss: 1.5467 - val_acc: 0.4210\n",
      "Epoch 72/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.5575 - acc: 0.4130 - val_loss: 1.5431 - val_acc: 0.4360\n",
      "Epoch 73/500\n",
      "1000/1000 [==============================] - 1s 670us/step - loss: 1.5370 - acc: 0.3920 - val_loss: 1.5425 - val_acc: 0.4270\n",
      "Epoch 74/500\n",
      "1000/1000 [==============================] - 1s 705us/step - loss: 1.5231 - acc: 0.3920 - val_loss: 1.5365 - val_acc: 0.4330\n",
      "Epoch 75/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.5347 - acc: 0.4080 - val_loss: 1.5354 - val_acc: 0.4280\n",
      "Epoch 76/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.5074 - acc: 0.4360 - val_loss: 1.5355 - val_acc: 0.4150\n",
      "Epoch 77/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.5179 - acc: 0.4250 - val_loss: 1.5360 - val_acc: 0.4150\n",
      "Epoch 78/500\n",
      "1000/1000 [==============================] - 1s 704us/step - loss: 1.5085 - acc: 0.4140 - val_loss: 1.5344 - val_acc: 0.4420\n",
      "Epoch 79/500\n",
      "1000/1000 [==============================] - 1s 667us/step - loss: 1.5293 - acc: 0.4230 - val_loss: 1.5533 - val_acc: 0.3750\n",
      "Epoch 80/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.5034 - acc: 0.4210 - val_loss: 1.5514 - val_acc: 0.4100\n",
      "Epoch 81/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.5071 - acc: 0.4320 - val_loss: 1.5323 - val_acc: 0.4370\n",
      "Epoch 82/500\n",
      "1000/1000 [==============================] - 1s 669us/step - loss: 1.5027 - acc: 0.4350 - val_loss: 1.5317 - val_acc: 0.4490\n",
      "Epoch 83/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.4946 - acc: 0.4360 - val_loss: 1.5332 - val_acc: 0.4180\n",
      "Epoch 84/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.5025 - acc: 0.4420 - val_loss: 1.5370 - val_acc: 0.4220\n",
      "Epoch 85/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.5345 - acc: 0.3970 - val_loss: 1.5250 - val_acc: 0.4370\n",
      "Epoch 86/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.4946 - acc: 0.4360 - val_loss: 1.5250 - val_acc: 0.4340\n",
      "Epoch 87/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.5133 - acc: 0.4280 - val_loss: 1.5248 - val_acc: 0.4450\n",
      "Epoch 88/500\n",
      "1000/1000 [==============================] - 1s 665us/step - loss: 1.4853 - acc: 0.4430 - val_loss: 1.5251 - val_acc: 0.4390\n",
      "Epoch 89/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.5216 - acc: 0.4110 - val_loss: 1.5280 - val_acc: 0.4410\n",
      "Epoch 90/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 1.5198 - acc: 0.4230 - val_loss: 1.5243 - val_acc: 0.4430\n",
      "Epoch 91/500\n",
      "1000/1000 [==============================] - 1s 661us/step - loss: 1.5092 - acc: 0.4290 - val_loss: 1.5264 - val_acc: 0.4290\n",
      "Epoch 92/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.5199 - acc: 0.4380 - val_loss: 1.5300 - val_acc: 0.4400\n",
      "Epoch 93/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.5096 - acc: 0.4300 - val_loss: 1.5265 - val_acc: 0.4400\n",
      "Epoch 94/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.5000 - acc: 0.4400 - val_loss: 1.5239 - val_acc: 0.4470\n",
      "Epoch 95/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.5038 - acc: 0.4420 - val_loss: 1.5241 - val_acc: 0.4620\n",
      "Epoch 96/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 1.4946 - acc: 0.4490 - val_loss: 1.5172 - val_acc: 0.4570\n",
      "Epoch 97/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.4889 - acc: 0.4410 - val_loss: 1.5193 - val_acc: 0.4550\n",
      "Epoch 98/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.5093 - acc: 0.4320 - val_loss: 1.5224 - val_acc: 0.4370\n",
      "Epoch 99/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.5097 - acc: 0.4220 - val_loss: 1.5204 - val_acc: 0.4370\n",
      "Epoch 100/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.4986 - acc: 0.4480 - val_loss: 1.5175 - val_acc: 0.4600\n",
      "Epoch 101/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.5007 - acc: 0.4500 - val_loss: 1.5158 - val_acc: 0.4420\n",
      "Epoch 102/500\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 1.4851 - acc: 0.4470 - val_loss: 1.5198 - val_acc: 0.4320\n",
      "Epoch 103/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.4925 - acc: 0.4600 - val_loss: 1.5200 - val_acc: 0.4560\n",
      "Epoch 104/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.4731 - acc: 0.4500 - val_loss: 1.5164 - val_acc: 0.4770\n",
      "Epoch 105/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.4866 - acc: 0.4430 - val_loss: 1.5125 - val_acc: 0.4520\n",
      "Epoch 106/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.4984 - acc: 0.4310 - val_loss: 1.5110 - val_acc: 0.4690\n",
      "Epoch 107/500\n",
      "1000/1000 [==============================] - 1s 706us/step - loss: 1.4758 - acc: 0.4560 - val_loss: 1.5131 - val_acc: 0.4580\n",
      "Epoch 108/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.4771 - acc: 0.4360 - val_loss: 1.5283 - val_acc: 0.4280\n",
      "Epoch 109/500\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 1.4602 - acc: 0.4720 - val_loss: 1.5068 - val_acc: 0.4680\n",
      "Epoch 110/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.4990 - acc: 0.4490 - val_loss: 1.5080 - val_acc: 0.4730\n",
      "Epoch 111/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.5068 - acc: 0.4390 - val_loss: 1.5117 - val_acc: 0.4520\n",
      "Epoch 112/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.4423 - acc: 0.4640 - val_loss: 1.5102 - val_acc: 0.4580\n",
      "Epoch 113/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.4799 - acc: 0.4710 - val_loss: 1.4989 - val_acc: 0.4730\n",
      "Epoch 114/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.4597 - acc: 0.4620 - val_loss: 1.5010 - val_acc: 0.4640\n",
      "Epoch 115/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.5050 - acc: 0.4590 - val_loss: 1.5017 - val_acc: 0.4700\n",
      "Epoch 116/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.4717 - acc: 0.4540 - val_loss: 1.5114 - val_acc: 0.4610\n",
      "Epoch 117/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.4510 - acc: 0.4650 - val_loss: 1.4967 - val_acc: 0.4710\n",
      "Epoch 118/500\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 1.4591 - acc: 0.4670 - val_loss: 1.5028 - val_acc: 0.4620\n",
      "Epoch 119/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 1.4694 - acc: 0.4710 - val_loss: 1.5003 - val_acc: 0.4720\n",
      "Epoch 120/500\n",
      "1000/1000 [==============================] - 1s 711us/step - loss: 1.4564 - acc: 0.4700 - val_loss: 1.5041 - val_acc: 0.4740\n",
      "Epoch 121/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 1.4629 - acc: 0.4700 - val_loss: 1.4992 - val_acc: 0.4720\n",
      "Epoch 122/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.4567 - acc: 0.4630 - val_loss: 1.4990 - val_acc: 0.4700\n",
      "Epoch 123/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.4482 - acc: 0.4830 - val_loss: 1.4991 - val_acc: 0.4710\n",
      "Epoch 124/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.4558 - acc: 0.4600 - val_loss: 1.4958 - val_acc: 0.4790\n",
      "Epoch 125/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.4471 - acc: 0.4680 - val_loss: 1.4938 - val_acc: 0.4760\n",
      "Epoch 126/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.4783 - acc: 0.4780 - val_loss: 1.4929 - val_acc: 0.4700\n",
      "Epoch 127/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.4612 - acc: 0.4650 - val_loss: 1.4930 - val_acc: 0.4860\n",
      "Epoch 128/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.4280 - acc: 0.4780 - val_loss: 1.4913 - val_acc: 0.4720\n",
      "Epoch 129/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.4460 - acc: 0.4680 - val_loss: 1.4870 - val_acc: 0.4880\n",
      "Epoch 130/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.4459 - acc: 0.4620 - val_loss: 1.4865 - val_acc: 0.4850\n",
      "Epoch 131/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.4425 - acc: 0.4780 - val_loss: 1.4892 - val_acc: 0.4770\n",
      "Epoch 132/500\n",
      "1000/1000 [==============================] - 1s 704us/step - loss: 1.4402 - acc: 0.4700 - val_loss: 1.4788 - val_acc: 0.4790\n",
      "Epoch 133/500\n",
      "1000/1000 [==============================] - 1s 703us/step - loss: 1.4275 - acc: 0.4790 - val_loss: 1.4783 - val_acc: 0.4850\n",
      "Epoch 134/500\n",
      "1000/1000 [==============================] - 1s 668us/step - loss: 1.4244 - acc: 0.4830 - val_loss: 1.4810 - val_acc: 0.4860\n",
      "Epoch 135/500\n",
      "1000/1000 [==============================] - 1s 659us/step - loss: 1.4292 - acc: 0.4840 - val_loss: 1.4880 - val_acc: 0.4850\n",
      "Epoch 136/500\n",
      "1000/1000 [==============================] - 1s 656us/step - loss: 1.4228 - acc: 0.4740 - val_loss: 1.4810 - val_acc: 0.4880\n",
      "Epoch 137/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 1.4291 - acc: 0.4810 - val_loss: 1.4786 - val_acc: 0.4910\n",
      "Epoch 138/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.4385 - acc: 0.4600 - val_loss: 1.4813 - val_acc: 0.4910\n",
      "Epoch 139/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.4274 - acc: 0.4600 - val_loss: 1.4719 - val_acc: 0.4830\n",
      "Epoch 140/500\n",
      "1000/1000 [==============================] - 1s 707us/step - loss: 1.4334 - acc: 0.4830 - val_loss: 1.4727 - val_acc: 0.4880\n",
      "Epoch 141/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.4155 - acc: 0.4780 - val_loss: 1.4747 - val_acc: 0.4920\n",
      "Epoch 142/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.4502 - acc: 0.4680 - val_loss: 1.4651 - val_acc: 0.4780\n",
      "Epoch 143/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.4320 - acc: 0.4630 - val_loss: 1.4670 - val_acc: 0.4900\n",
      "Epoch 144/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.4212 - acc: 0.4950 - val_loss: 1.4746 - val_acc: 0.4780\n",
      "Epoch 145/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.4237 - acc: 0.4780 - val_loss: 1.4639 - val_acc: 0.4980\n",
      "Epoch 146/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.4145 - acc: 0.4780 - val_loss: 1.4616 - val_acc: 0.4860\n",
      "Epoch 147/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.4270 - acc: 0.4680 - val_loss: 1.4618 - val_acc: 0.4850\n",
      "Epoch 148/500\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 1.4173 - acc: 0.4840 - val_loss: 1.4534 - val_acc: 0.4830\n",
      "Epoch 149/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.4199 - acc: 0.4740 - val_loss: 1.4613 - val_acc: 0.4990\n",
      "Epoch 150/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.4079 - acc: 0.5030 - val_loss: 1.4546 - val_acc: 0.4950\n",
      "Epoch 151/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.4096 - acc: 0.4930 - val_loss: 1.4590 - val_acc: 0.4950\n",
      "Epoch 152/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.4157 - acc: 0.4890 - val_loss: 1.4563 - val_acc: 0.5030\n",
      "Epoch 153/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.4178 - acc: 0.4870 - val_loss: 1.4555 - val_acc: 0.4880\n",
      "Epoch 154/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.4044 - acc: 0.4960 - val_loss: 1.4594 - val_acc: 0.4990\n",
      "Epoch 155/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.4268 - acc: 0.4630 - val_loss: 1.4572 - val_acc: 0.5000\n",
      "Epoch 156/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.3957 - acc: 0.4770 - val_loss: 1.4593 - val_acc: 0.5000\n",
      "Epoch 157/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.3962 - acc: 0.5050 - val_loss: 1.4509 - val_acc: 0.4770\n",
      "Epoch 158/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.3993 - acc: 0.4760 - val_loss: 1.4494 - val_acc: 0.5040\n",
      "Epoch 159/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.4143 - acc: 0.5040 - val_loss: 1.4478 - val_acc: 0.4970\n",
      "Epoch 160/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.3952 - acc: 0.4850 - val_loss: 1.4501 - val_acc: 0.4920\n",
      "Epoch 161/500\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 1.3889 - acc: 0.4850 - val_loss: 1.4492 - val_acc: 0.4950\n",
      "Epoch 162/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.4044 - acc: 0.4850 - val_loss: 1.4468 - val_acc: 0.4750\n",
      "Epoch 163/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.3989 - acc: 0.4780 - val_loss: 1.4510 - val_acc: 0.4980\n",
      "Epoch 164/500\n",
      "1000/1000 [==============================] - 1s 666us/step - loss: 1.4139 - acc: 0.4780 - val_loss: 1.4498 - val_acc: 0.4860\n",
      "Epoch 165/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.4201 - acc: 0.4910 - val_loss: 1.4522 - val_acc: 0.4990\n",
      "Epoch 166/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.4002 - acc: 0.4960 - val_loss: 1.4403 - val_acc: 0.4730\n",
      "Epoch 167/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.3631 - acc: 0.5070 - val_loss: 1.4451 - val_acc: 0.4910\n",
      "Epoch 168/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.4049 - acc: 0.4970 - val_loss: 1.4436 - val_acc: 0.4980\n",
      "Epoch 169/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.3759 - acc: 0.5000 - val_loss: 1.4442 - val_acc: 0.4890\n",
      "Epoch 170/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.3800 - acc: 0.4780 - val_loss: 1.4431 - val_acc: 0.4990\n",
      "Epoch 171/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 1.3644 - acc: 0.4950 - val_loss: 1.4374 - val_acc: 0.4910\n",
      "Epoch 172/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.4047 - acc: 0.4770 - val_loss: 1.4360 - val_acc: 0.4760\n",
      "Epoch 173/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.4069 - acc: 0.4880 - val_loss: 1.4390 - val_acc: 0.5030\n",
      "Epoch 174/500\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 1.3749 - acc: 0.4970 - val_loss: 1.4381 - val_acc: 0.4660\n",
      "Epoch 175/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.3695 - acc: 0.4950 - val_loss: 1.4442 - val_acc: 0.5000\n",
      "Epoch 176/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.3819 - acc: 0.4860 - val_loss: 1.4303 - val_acc: 0.4860\n",
      "Epoch 177/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.3701 - acc: 0.4960 - val_loss: 1.4327 - val_acc: 0.5050\n",
      "Epoch 178/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.3447 - acc: 0.5020 - val_loss: 1.4317 - val_acc: 0.5040\n",
      "Epoch 179/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.3966 - acc: 0.4890 - val_loss: 1.4275 - val_acc: 0.4950\n",
      "Epoch 180/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.3832 - acc: 0.4920 - val_loss: 1.4306 - val_acc: 0.4900\n",
      "Epoch 181/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.3774 - acc: 0.4920 - val_loss: 1.4325 - val_acc: 0.5080\n",
      "Epoch 182/500\n",
      "1000/1000 [==============================] - 1s 662us/step - loss: 1.3729 - acc: 0.4940 - val_loss: 1.4376 - val_acc: 0.5140\n",
      "Epoch 183/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.3702 - acc: 0.4860 - val_loss: 1.4361 - val_acc: 0.5000\n",
      "Epoch 184/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.3787 - acc: 0.4980 - val_loss: 1.4266 - val_acc: 0.4970\n",
      "Epoch 185/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.3898 - acc: 0.4840 - val_loss: 1.4289 - val_acc: 0.4980\n",
      "Epoch 186/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.3661 - acc: 0.5010 - val_loss: 1.4242 - val_acc: 0.5030\n",
      "Epoch 187/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.3824 - acc: 0.5030 - val_loss: 1.4277 - val_acc: 0.4960\n",
      "Epoch 188/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.3801 - acc: 0.4960 - val_loss: 1.4419 - val_acc: 0.5080\n",
      "Epoch 189/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.3808 - acc: 0.4860 - val_loss: 1.4298 - val_acc: 0.5050\n",
      "Epoch 190/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.3553 - acc: 0.5240 - val_loss: 1.4260 - val_acc: 0.5040\n",
      "Epoch 191/500\n",
      "1000/1000 [==============================] - 1s 666us/step - loss: 1.4095 - acc: 0.4970 - val_loss: 1.4263 - val_acc: 0.5110\n",
      "Epoch 192/500\n",
      "1000/1000 [==============================] - 1s 668us/step - loss: 1.3452 - acc: 0.5060 - val_loss: 1.4327 - val_acc: 0.5030\n",
      "Epoch 193/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.3670 - acc: 0.5030 - val_loss: 1.4232 - val_acc: 0.5070\n",
      "Epoch 194/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 1.3673 - acc: 0.4970 - val_loss: 1.4263 - val_acc: 0.5060\n",
      "Epoch 195/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.3508 - acc: 0.5040 - val_loss: 1.4283 - val_acc: 0.4920\n",
      "Epoch 196/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 1.3598 - acc: 0.5000 - val_loss: 1.4247 - val_acc: 0.5090\n",
      "Epoch 197/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 1.3768 - acc: 0.5020 - val_loss: 1.4253 - val_acc: 0.5060\n",
      "Epoch 198/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 1.3760 - acc: 0.4910 - val_loss: 1.4185 - val_acc: 0.5050\n",
      "Epoch 199/500\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 1.3370 - acc: 0.5210 - val_loss: 1.4196 - val_acc: 0.5110\n",
      "Epoch 200/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 1.3505 - acc: 0.5020 - val_loss: 1.4224 - val_acc: 0.5000\n",
      "Epoch 201/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.3346 - acc: 0.5120 - val_loss: 1.4225 - val_acc: 0.5110\n",
      "Epoch 202/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.3360 - acc: 0.5320 - val_loss: 1.4133 - val_acc: 0.5090\n",
      "Epoch 203/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.3595 - acc: 0.4860 - val_loss: 1.4157 - val_acc: 0.5060\n",
      "Epoch 204/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.3628 - acc: 0.5070 - val_loss: 1.4217 - val_acc: 0.4950\n",
      "Epoch 205/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.3718 - acc: 0.4930 - val_loss: 1.4240 - val_acc: 0.4990\n",
      "Epoch 206/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.3715 - acc: 0.5080 - val_loss: 1.4295 - val_acc: 0.5080\n",
      "Epoch 207/500\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 1.3411 - acc: 0.5060 - val_loss: 1.4225 - val_acc: 0.5150\n",
      "Epoch 208/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.3498 - acc: 0.5190 - val_loss: 1.4246 - val_acc: 0.5170\n",
      "Epoch 209/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.3445 - acc: 0.5270 - val_loss: 1.4109 - val_acc: 0.5080\n",
      "Epoch 210/500\n",
      "1000/1000 [==============================] - 1s 710us/step - loss: 1.3528 - acc: 0.5040 - val_loss: 1.4076 - val_acc: 0.5120\n",
      "Epoch 211/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.3142 - acc: 0.5220 - val_loss: 1.4156 - val_acc: 0.5140\n",
      "Epoch 212/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 1.3194 - acc: 0.5130 - val_loss: 1.4127 - val_acc: 0.5170\n",
      "Epoch 213/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.3493 - acc: 0.5140 - val_loss: 1.4144 - val_acc: 0.5100\n",
      "Epoch 214/500\n",
      "1000/1000 [==============================] - 1s 716us/step - loss: 1.3314 - acc: 0.5110 - val_loss: 1.4116 - val_acc: 0.5100\n",
      "Epoch 215/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.3238 - acc: 0.5160 - val_loss: 1.4084 - val_acc: 0.5110\n",
      "Epoch 216/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.3340 - acc: 0.5240 - val_loss: 1.4112 - val_acc: 0.5030\n",
      "Epoch 217/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.3493 - acc: 0.5140 - val_loss: 1.4095 - val_acc: 0.5040\n",
      "Epoch 218/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.3251 - acc: 0.5110 - val_loss: 1.4096 - val_acc: 0.5110\n",
      "Epoch 219/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 1.3396 - acc: 0.5100 - val_loss: 1.4112 - val_acc: 0.5080\n",
      "Epoch 220/500\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 1.3117 - acc: 0.5240 - val_loss: 1.4067 - val_acc: 0.5090\n",
      "Epoch 221/500\n",
      "1000/1000 [==============================] - 1s 670us/step - loss: 1.3424 - acc: 0.5140 - val_loss: 1.4013 - val_acc: 0.5080\n",
      "Epoch 222/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.3414 - acc: 0.5080 - val_loss: 1.4009 - val_acc: 0.5080\n",
      "Epoch 223/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.3393 - acc: 0.5120 - val_loss: 1.4052 - val_acc: 0.5170\n",
      "Epoch 224/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.3260 - acc: 0.5030 - val_loss: 1.4047 - val_acc: 0.4890\n",
      "Epoch 225/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.3037 - acc: 0.5210 - val_loss: 1.4024 - val_acc: 0.5160\n",
      "Epoch 226/500\n",
      "1000/1000 [==============================] - 1s 705us/step - loss: 1.3183 - acc: 0.5250 - val_loss: 1.3948 - val_acc: 0.5080\n",
      "Epoch 227/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.3365 - acc: 0.5120 - val_loss: 1.4025 - val_acc: 0.4920\n",
      "Epoch 228/500\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 1.2890 - acc: 0.5310 - val_loss: 1.3946 - val_acc: 0.5150\n",
      "Epoch 229/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.3239 - acc: 0.5140 - val_loss: 1.4016 - val_acc: 0.5130\n",
      "Epoch 230/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.3174 - acc: 0.5170 - val_loss: 1.3938 - val_acc: 0.5110\n",
      "Epoch 231/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.2762 - acc: 0.5240 - val_loss: 1.3933 - val_acc: 0.5140\n",
      "Epoch 232/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.3206 - acc: 0.5150 - val_loss: 1.3895 - val_acc: 0.5100\n",
      "Epoch 233/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.3030 - acc: 0.5180 - val_loss: 1.3873 - val_acc: 0.5110\n",
      "Epoch 234/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.3075 - acc: 0.5110 - val_loss: 1.3939 - val_acc: 0.5190\n",
      "Epoch 235/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.3034 - acc: 0.5360 - val_loss: 1.3911 - val_acc: 0.5180\n",
      "Epoch 236/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.2830 - acc: 0.5370 - val_loss: 1.3894 - val_acc: 0.5140\n",
      "Epoch 237/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.3010 - acc: 0.5190 - val_loss: 1.3826 - val_acc: 0.5110\n",
      "Epoch 238/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.2947 - acc: 0.5300 - val_loss: 1.3904 - val_acc: 0.5170\n",
      "Epoch 239/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.2895 - acc: 0.5230 - val_loss: 1.3872 - val_acc: 0.5210\n",
      "Epoch 240/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.3129 - acc: 0.5150 - val_loss: 1.3807 - val_acc: 0.5180\n",
      "Epoch 241/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.2728 - acc: 0.5330 - val_loss: 1.3976 - val_acc: 0.5210\n",
      "Epoch 242/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.2862 - acc: 0.5200 - val_loss: 1.3654 - val_acc: 0.5260\n",
      "Epoch 243/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.2701 - acc: 0.5290 - val_loss: 1.3744 - val_acc: 0.5050\n",
      "Epoch 244/500\n",
      "1000/1000 [==============================] - 1s 709us/step - loss: 1.2967 - acc: 0.5210 - val_loss: 1.3703 - val_acc: 0.5220\n",
      "Epoch 245/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 1.2821 - acc: 0.5150 - val_loss: 1.3713 - val_acc: 0.5010\n",
      "Epoch 246/500\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 1.2819 - acc: 0.5230 - val_loss: 1.3743 - val_acc: 0.5260\n",
      "Epoch 247/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.2619 - acc: 0.5360 - val_loss: 1.3574 - val_acc: 0.5090\n",
      "Epoch 248/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.2957 - acc: 0.5240 - val_loss: 1.3645 - val_acc: 0.5080\n",
      "Epoch 249/500\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 1.2768 - acc: 0.5390 - val_loss: 1.3538 - val_acc: 0.5110\n",
      "Epoch 250/500\n",
      "1000/1000 [==============================] - 1s 706us/step - loss: 1.2578 - acc: 0.5360 - val_loss: 1.3547 - val_acc: 0.5260\n",
      "Epoch 251/500\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 1.2147 - acc: 0.5680 - val_loss: 1.3521 - val_acc: 0.5240\n",
      "Epoch 252/500\n",
      "1000/1000 [==============================] - 1s 669us/step - loss: 1.2678 - acc: 0.5380 - val_loss: 1.3455 - val_acc: 0.5220\n",
      "Epoch 253/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.2582 - acc: 0.5340 - val_loss: 1.3512 - val_acc: 0.5250\n",
      "Epoch 254/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.2676 - acc: 0.5440 - val_loss: 1.3539 - val_acc: 0.5340\n",
      "Epoch 255/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.2223 - acc: 0.5590 - val_loss: 1.3549 - val_acc: 0.5110\n",
      "Epoch 256/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.2903 - acc: 0.5250 - val_loss: 1.3433 - val_acc: 0.5320\n",
      "Epoch 257/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.2584 - acc: 0.5420 - val_loss: 1.3411 - val_acc: 0.5360\n",
      "Epoch 258/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 1.2626 - acc: 0.5240 - val_loss: 1.3416 - val_acc: 0.5310\n",
      "Epoch 259/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 1.2511 - acc: 0.5270 - val_loss: 1.3297 - val_acc: 0.5410\n",
      "Epoch 260/500\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 1.2160 - acc: 0.5760 - val_loss: 1.3304 - val_acc: 0.5390\n",
      "Epoch 261/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.2392 - acc: 0.5410 - val_loss: 1.3206 - val_acc: 0.5390\n",
      "Epoch 262/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.2235 - acc: 0.5560 - val_loss: 1.3298 - val_acc: 0.5300\n",
      "Epoch 263/500\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 1.2435 - acc: 0.5540 - val_loss: 1.3267 - val_acc: 0.5360\n",
      "Epoch 264/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.2163 - acc: 0.5580 - val_loss: 1.3308 - val_acc: 0.5380\n",
      "Epoch 265/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.2428 - acc: 0.5450 - val_loss: 1.3192 - val_acc: 0.5290\n",
      "Epoch 266/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.2286 - acc: 0.5510 - val_loss: 1.3195 - val_acc: 0.5370\n",
      "Epoch 267/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.2301 - acc: 0.5500 - val_loss: 1.3322 - val_acc: 0.5310\n",
      "Epoch 268/500\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 1.2198 - acc: 0.5780 - val_loss: 1.3281 - val_acc: 0.5330\n",
      "Epoch 269/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.2225 - acc: 0.5500 - val_loss: 1.3167 - val_acc: 0.5360\n",
      "Epoch 270/500\n",
      "1000/1000 [==============================] - 1s 664us/step - loss: 1.1884 - acc: 0.5720 - val_loss: 1.3195 - val_acc: 0.5290\n",
      "Epoch 271/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.2035 - acc: 0.5890 - val_loss: 1.3169 - val_acc: 0.5300\n",
      "Epoch 272/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 1.2276 - acc: 0.5540 - val_loss: 1.3131 - val_acc: 0.5380\n",
      "Epoch 273/500\n",
      "1000/1000 [==============================] - 1s 658us/step - loss: 1.2095 - acc: 0.5550 - val_loss: 1.3054 - val_acc: 0.5310\n",
      "Epoch 274/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.1978 - acc: 0.5660 - val_loss: 1.3028 - val_acc: 0.5410\n",
      "Epoch 275/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.2199 - acc: 0.5440 - val_loss: 1.3044 - val_acc: 0.5500\n",
      "Epoch 276/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.2017 - acc: 0.5620 - val_loss: 1.2903 - val_acc: 0.5460\n",
      "Epoch 277/500\n",
      "1000/1000 [==============================] - 1s 703us/step - loss: 1.2102 - acc: 0.5770 - val_loss: 1.2957 - val_acc: 0.5450\n",
      "Epoch 278/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.1925 - acc: 0.5720 - val_loss: 1.2880 - val_acc: 0.5520\n",
      "Epoch 279/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.1724 - acc: 0.5750 - val_loss: 1.2887 - val_acc: 0.5510\n",
      "Epoch 280/500\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 1.1971 - acc: 0.5670 - val_loss: 1.2906 - val_acc: 0.5420\n",
      "Epoch 281/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.1758 - acc: 0.5850 - val_loss: 1.2854 - val_acc: 0.5540\n",
      "Epoch 282/500\n",
      "1000/1000 [==============================] - 1s 673us/step - loss: 1.1457 - acc: 0.5860 - val_loss: 1.2881 - val_acc: 0.5510\n",
      "Epoch 283/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.1819 - acc: 0.5720 - val_loss: 1.2856 - val_acc: 0.5530\n",
      "Epoch 284/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.1615 - acc: 0.5860 - val_loss: 1.2800 - val_acc: 0.5570\n",
      "Epoch 285/500\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 1.1764 - acc: 0.5770 - val_loss: 1.2762 - val_acc: 0.5500\n",
      "Epoch 286/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.1621 - acc: 0.5880 - val_loss: 1.2768 - val_acc: 0.5590\n",
      "Epoch 287/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 1.1611 - acc: 0.5800 - val_loss: 1.2613 - val_acc: 0.5610\n",
      "Epoch 288/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.1740 - acc: 0.5730 - val_loss: 1.2671 - val_acc: 0.5550\n",
      "Epoch 289/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.1777 - acc: 0.5980 - val_loss: 1.2669 - val_acc: 0.5620\n",
      "Epoch 290/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 1.1586 - acc: 0.5900 - val_loss: 1.2702 - val_acc: 0.5620\n",
      "Epoch 291/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.1414 - acc: 0.5880 - val_loss: 1.2564 - val_acc: 0.5620\n",
      "Epoch 292/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.1520 - acc: 0.5770 - val_loss: 1.2562 - val_acc: 0.5630\n",
      "Epoch 293/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 1.1325 - acc: 0.5840 - val_loss: 1.2724 - val_acc: 0.5600\n",
      "Epoch 294/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.1408 - acc: 0.5880 - val_loss: 1.2633 - val_acc: 0.5600\n",
      "Epoch 295/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 1.1490 - acc: 0.6010 - val_loss: 1.2780 - val_acc: 0.5600\n",
      "Epoch 296/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.1426 - acc: 0.5840 - val_loss: 1.2537 - val_acc: 0.5650\n",
      "Epoch 297/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 1.1412 - acc: 0.5880 - val_loss: 1.2541 - val_acc: 0.5680\n",
      "Epoch 298/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.1139 - acc: 0.5990 - val_loss: 1.2652 - val_acc: 0.5620\n",
      "Epoch 299/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.1122 - acc: 0.5990 - val_loss: 1.2516 - val_acc: 0.5640\n",
      "Epoch 300/500\n",
      "1000/1000 [==============================] - 1s 667us/step - loss: 1.1217 - acc: 0.6080 - val_loss: 1.2609 - val_acc: 0.5640\n",
      "Epoch 301/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.1496 - acc: 0.5650 - val_loss: 1.2434 - val_acc: 0.5710\n",
      "Epoch 302/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.1003 - acc: 0.5960 - val_loss: 1.2469 - val_acc: 0.5770\n",
      "Epoch 303/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 1.1267 - acc: 0.6080 - val_loss: 1.2476 - val_acc: 0.5760\n",
      "Epoch 304/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.1062 - acc: 0.6010 - val_loss: 1.2584 - val_acc: 0.5720\n",
      "Epoch 305/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.1119 - acc: 0.6030 - val_loss: 1.2523 - val_acc: 0.5760\n",
      "Epoch 306/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.1007 - acc: 0.6030 - val_loss: 1.2484 - val_acc: 0.5760\n",
      "Epoch 307/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.1201 - acc: 0.5930 - val_loss: 1.2482 - val_acc: 0.5700\n",
      "Epoch 308/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.1237 - acc: 0.5670 - val_loss: 1.2432 - val_acc: 0.5740\n",
      "Epoch 309/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.0872 - acc: 0.6210 - val_loss: 1.2356 - val_acc: 0.5760\n",
      "Epoch 310/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.0937 - acc: 0.6190 - val_loss: 1.2467 - val_acc: 0.5780\n",
      "Epoch 311/500\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 1.0961 - acc: 0.6100 - val_loss: 1.2273 - val_acc: 0.5840\n",
      "Epoch 312/500\n",
      "1000/1000 [==============================] - 1s 670us/step - loss: 1.1577 - acc: 0.5820 - val_loss: 1.2182 - val_acc: 0.5830\n",
      "Epoch 313/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 1.1000 - acc: 0.6160 - val_loss: 1.2258 - val_acc: 0.5800\n",
      "Epoch 314/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.1037 - acc: 0.5980 - val_loss: 1.2290 - val_acc: 0.5800\n",
      "Epoch 315/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.1016 - acc: 0.6170 - val_loss: 1.2316 - val_acc: 0.5770\n",
      "Epoch 316/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 1.0708 - acc: 0.6150 - val_loss: 1.2285 - val_acc: 0.5830\n",
      "Epoch 317/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 1.0882 - acc: 0.6120 - val_loss: 1.2068 - val_acc: 0.5900\n",
      "Epoch 318/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.0886 - acc: 0.6160 - val_loss: 1.2302 - val_acc: 0.5770\n",
      "Epoch 319/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.0523 - acc: 0.6140 - val_loss: 1.2230 - val_acc: 0.5870\n",
      "Epoch 320/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.0708 - acc: 0.6090 - val_loss: 1.2183 - val_acc: 0.5810\n",
      "Epoch 321/500\n",
      "1000/1000 [==============================] - 1s 673us/step - loss: 1.0754 - acc: 0.6140 - val_loss: 1.2121 - val_acc: 0.5950\n",
      "Epoch 322/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 1.0775 - acc: 0.6180 - val_loss: 1.2147 - val_acc: 0.5880\n",
      "Epoch 323/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.0836 - acc: 0.6130 - val_loss: 1.2199 - val_acc: 0.5830\n",
      "Epoch 324/500\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 1.0450 - acc: 0.6310 - val_loss: 1.2123 - val_acc: 0.5870\n",
      "Epoch 325/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.0654 - acc: 0.6200 - val_loss: 1.2123 - val_acc: 0.5840\n",
      "Epoch 326/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.0943 - acc: 0.5980 - val_loss: 1.1983 - val_acc: 0.5940\n",
      "Epoch 327/500\n",
      "1000/1000 [==============================] - 1s 659us/step - loss: 1.0380 - acc: 0.6360 - val_loss: 1.2020 - val_acc: 0.5950\n",
      "Epoch 328/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.0424 - acc: 0.6240 - val_loss: 1.2129 - val_acc: 0.5930\n",
      "Epoch 329/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.0557 - acc: 0.6370 - val_loss: 1.2025 - val_acc: 0.5960\n",
      "Epoch 330/500\n",
      "1000/1000 [==============================] - 1s 665us/step - loss: 1.0539 - acc: 0.6350 - val_loss: 1.1916 - val_acc: 0.6020\n",
      "Epoch 331/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.0647 - acc: 0.6170 - val_loss: 1.1901 - val_acc: 0.6000\n",
      "Epoch 332/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 1.0738 - acc: 0.6220 - val_loss: 1.1897 - val_acc: 0.5950\n",
      "Epoch 333/500\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 1.0316 - acc: 0.6360 - val_loss: 1.1934 - val_acc: 0.5860\n",
      "Epoch 334/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.0387 - acc: 0.6260 - val_loss: 1.1784 - val_acc: 0.6020\n",
      "Epoch 335/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 1.0469 - acc: 0.6310 - val_loss: 1.1885 - val_acc: 0.6010\n",
      "Epoch 336/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 1.0546 - acc: 0.6360 - val_loss: 1.1920 - val_acc: 0.6040\n",
      "Epoch 337/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.0358 - acc: 0.6440 - val_loss: 1.1815 - val_acc: 0.6070\n",
      "Epoch 338/500\n",
      "1000/1000 [==============================] - 1s 719us/step - loss: 1.0579 - acc: 0.6300 - val_loss: 1.1777 - val_acc: 0.6090\n",
      "Epoch 339/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.0505 - acc: 0.6290 - val_loss: 1.1793 - val_acc: 0.6020\n",
      "Epoch 340/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 1.0291 - acc: 0.6260 - val_loss: 1.1877 - val_acc: 0.6070\n",
      "Epoch 341/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 1.0428 - acc: 0.6250 - val_loss: 1.1851 - val_acc: 0.6020\n",
      "Epoch 342/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.0566 - acc: 0.6390 - val_loss: 1.1741 - val_acc: 0.6030\n",
      "Epoch 343/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.9982 - acc: 0.6640 - val_loss: 1.1807 - val_acc: 0.6040\n",
      "Epoch 344/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.0111 - acc: 0.6400 - val_loss: 1.1728 - val_acc: 0.6060\n",
      "Epoch 345/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 1.0534 - acc: 0.6290 - val_loss: 1.1725 - val_acc: 0.6130\n",
      "Epoch 346/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.0235 - acc: 0.6520 - val_loss: 1.1783 - val_acc: 0.6130\n",
      "Epoch 347/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 1.0241 - acc: 0.6510 - val_loss: 1.1861 - val_acc: 0.6010\n",
      "Epoch 348/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 1.0349 - acc: 0.6240 - val_loss: 1.1752 - val_acc: 0.6010\n",
      "Epoch 349/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 1.0194 - acc: 0.6360 - val_loss: 1.1865 - val_acc: 0.6030\n",
      "Epoch 350/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 1.0329 - acc: 0.6270 - val_loss: 1.1813 - val_acc: 0.5990\n",
      "Epoch 351/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.0179 - acc: 0.6350 - val_loss: 1.1692 - val_acc: 0.6070\n",
      "Epoch 352/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 0.9894 - acc: 0.6520 - val_loss: 1.1698 - val_acc: 0.6090\n",
      "Epoch 353/500\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 1.0067 - acc: 0.6400 - val_loss: 1.1673 - val_acc: 0.6150\n",
      "Epoch 354/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 1.0090 - acc: 0.6520 - val_loss: 1.1609 - val_acc: 0.6070\n",
      "Epoch 355/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.0189 - acc: 0.6320 - val_loss: 1.1568 - val_acc: 0.6110\n",
      "Epoch 356/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.9946 - acc: 0.6490 - val_loss: 1.1532 - val_acc: 0.6220\n",
      "Epoch 357/500\n",
      "1000/1000 [==============================] - 1s 669us/step - loss: 1.0304 - acc: 0.6400 - val_loss: 1.1479 - val_acc: 0.6290\n",
      "Epoch 358/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 1.0094 - acc: 0.6450 - val_loss: 1.1626 - val_acc: 0.6220\n",
      "Epoch 359/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 1.0089 - acc: 0.6470 - val_loss: 1.1648 - val_acc: 0.6130\n",
      "Epoch 360/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 0.9846 - acc: 0.6480 - val_loss: 1.1569 - val_acc: 0.6120\n",
      "Epoch 361/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 1.0008 - acc: 0.6350 - val_loss: 1.1527 - val_acc: 0.6180\n",
      "Epoch 362/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 0.9952 - acc: 0.6590 - val_loss: 1.1593 - val_acc: 0.6130\n",
      "Epoch 363/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 1.0191 - acc: 0.6300 - val_loss: 1.1369 - val_acc: 0.6230\n",
      "Epoch 364/500\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 0.9657 - acc: 0.6540 - val_loss: 1.1415 - val_acc: 0.6170\n",
      "Epoch 365/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.9691 - acc: 0.6590 - val_loss: 1.1455 - val_acc: 0.6150\n",
      "Epoch 366/500\n",
      "1000/1000 [==============================] - 1s 659us/step - loss: 0.9594 - acc: 0.6600 - val_loss: 1.1340 - val_acc: 0.6120\n",
      "Epoch 367/500\n",
      "1000/1000 [==============================] - 1s 706us/step - loss: 0.9763 - acc: 0.6410 - val_loss: 1.1076 - val_acc: 0.6210\n",
      "Epoch 368/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.9645 - acc: 0.6660 - val_loss: 1.1011 - val_acc: 0.6260\n",
      "Epoch 369/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 0.9898 - acc: 0.6350 - val_loss: 1.0977 - val_acc: 0.6300\n",
      "Epoch 370/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 0.9662 - acc: 0.6590 - val_loss: 1.1044 - val_acc: 0.6290\n",
      "Epoch 371/500\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 0.9828 - acc: 0.6520 - val_loss: 1.1174 - val_acc: 0.6320\n",
      "Epoch 372/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 1.0151 - acc: 0.6410 - val_loss: 1.0933 - val_acc: 0.6240\n",
      "Epoch 373/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 0.9587 - acc: 0.6730 - val_loss: 1.0862 - val_acc: 0.6270\n",
      "Epoch 374/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 0.9652 - acc: 0.6620 - val_loss: 1.0939 - val_acc: 0.6270\n",
      "Epoch 375/500\n",
      "1000/1000 [==============================] - 1s 664us/step - loss: 0.9368 - acc: 0.6700 - val_loss: 1.0819 - val_acc: 0.6380\n",
      "Epoch 376/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.9537 - acc: 0.6720 - val_loss: 1.0777 - val_acc: 0.6340\n",
      "Epoch 377/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.9493 - acc: 0.6580 - val_loss: 1.0666 - val_acc: 0.6370\n",
      "Epoch 378/500\n",
      "1000/1000 [==============================] - 1s 673us/step - loss: 0.9730 - acc: 0.6590 - val_loss: 1.0849 - val_acc: 0.6320\n",
      "Epoch 379/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 0.9368 - acc: 0.6650 - val_loss: 1.0722 - val_acc: 0.6360\n",
      "Epoch 380/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 0.9805 - acc: 0.6680 - val_loss: 1.0572 - val_acc: 0.6450\n",
      "Epoch 381/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.9273 - acc: 0.6720 - val_loss: 1.0670 - val_acc: 0.6310\n",
      "Epoch 382/500\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 0.9456 - acc: 0.6750 - val_loss: 1.0529 - val_acc: 0.6500\n",
      "Epoch 383/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.9082 - acc: 0.6800 - val_loss: 1.0558 - val_acc: 0.6380\n",
      "Epoch 384/500\n",
      "1000/1000 [==============================] - 1s 670us/step - loss: 0.9552 - acc: 0.6580 - val_loss: 1.0454 - val_acc: 0.6450\n",
      "Epoch 385/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 0.9538 - acc: 0.6730 - val_loss: 1.0580 - val_acc: 0.6370\n",
      "Epoch 386/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.9297 - acc: 0.6740 - val_loss: 1.0412 - val_acc: 0.6540\n",
      "Epoch 387/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 0.9100 - acc: 0.6710 - val_loss: 1.0515 - val_acc: 0.6400\n",
      "Epoch 388/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 0.9214 - acc: 0.6740 - val_loss: 1.0337 - val_acc: 0.6450\n",
      "Epoch 389/500\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 0.9254 - acc: 0.6740 - val_loss: 1.0270 - val_acc: 0.6490\n",
      "Epoch 390/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 0.9386 - acc: 0.6670 - val_loss: 1.0384 - val_acc: 0.6390\n",
      "Epoch 391/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 0.9129 - acc: 0.6620 - val_loss: 1.0408 - val_acc: 0.6430\n",
      "Epoch 392/500\n",
      "1000/1000 [==============================] - 1s 665us/step - loss: 0.9157 - acc: 0.6860 - val_loss: 1.0370 - val_acc: 0.6420\n",
      "Epoch 393/500\n",
      "1000/1000 [==============================] - 1s 656us/step - loss: 0.9204 - acc: 0.6760 - val_loss: 1.0387 - val_acc: 0.6430\n",
      "Epoch 394/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 0.9617 - acc: 0.6680 - val_loss: 1.0171 - val_acc: 0.6490\n",
      "Epoch 395/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 0.8968 - acc: 0.6790 - val_loss: 1.0252 - val_acc: 0.6500\n",
      "Epoch 396/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.8967 - acc: 0.6780 - val_loss: 1.0302 - val_acc: 0.6530\n",
      "Epoch 397/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 0.9081 - acc: 0.6830 - val_loss: 1.0190 - val_acc: 0.6580\n",
      "Epoch 398/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 0.8915 - acc: 0.6760 - val_loss: 1.0148 - val_acc: 0.6580\n",
      "Epoch 399/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 0.9151 - acc: 0.6770 - val_loss: 1.0127 - val_acc: 0.6670\n",
      "Epoch 400/500\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 0.9001 - acc: 0.6750 - val_loss: 1.0180 - val_acc: 0.6460\n",
      "Epoch 401/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 0.9051 - acc: 0.6760 - val_loss: 1.0061 - val_acc: 0.6600\n",
      "Epoch 402/500\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 0.9120 - acc: 0.6750 - val_loss: 1.0068 - val_acc: 0.6510\n",
      "Epoch 403/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 0.9212 - acc: 0.6690 - val_loss: 1.0080 - val_acc: 0.6560\n",
      "Epoch 404/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.8946 - acc: 0.6890 - val_loss: 1.0080 - val_acc: 0.6570\n",
      "Epoch 405/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 0.8925 - acc: 0.6930 - val_loss: 1.0000 - val_acc: 0.6610\n",
      "Epoch 406/500\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 0.9194 - acc: 0.6690 - val_loss: 0.9934 - val_acc: 0.6590\n",
      "Epoch 407/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 0.9167 - acc: 0.6870 - val_loss: 0.9959 - val_acc: 0.6700\n",
      "Epoch 408/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.8730 - acc: 0.6900 - val_loss: 1.0015 - val_acc: 0.6630\n",
      "Epoch 409/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 0.9038 - acc: 0.6680 - val_loss: 1.0090 - val_acc: 0.6540\n",
      "Epoch 410/500\n",
      "1000/1000 [==============================] - 1s 673us/step - loss: 0.8534 - acc: 0.6920 - val_loss: 0.9993 - val_acc: 0.6620\n",
      "Epoch 411/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.8819 - acc: 0.6930 - val_loss: 0.9971 - val_acc: 0.6660\n",
      "Epoch 412/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 0.9124 - acc: 0.6850 - val_loss: 1.0106 - val_acc: 0.6530\n",
      "Epoch 413/500\n",
      "1000/1000 [==============================] - 1s 668us/step - loss: 0.8471 - acc: 0.6970 - val_loss: 0.9948 - val_acc: 0.6620\n",
      "Epoch 414/500\n",
      "1000/1000 [==============================] - 1s 662us/step - loss: 0.8997 - acc: 0.6860 - val_loss: 0.9921 - val_acc: 0.6670\n",
      "Epoch 415/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 0.8901 - acc: 0.6840 - val_loss: 0.9875 - val_acc: 0.6670\n",
      "Epoch 416/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.8565 - acc: 0.6890 - val_loss: 0.9737 - val_acc: 0.6690\n",
      "Epoch 417/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.8945 - acc: 0.6940 - val_loss: 1.0070 - val_acc: 0.6490\n",
      "Epoch 418/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 0.8748 - acc: 0.6980 - val_loss: 0.9811 - val_acc: 0.6660\n",
      "Epoch 419/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 0.8776 - acc: 0.6830 - val_loss: 0.9864 - val_acc: 0.6610\n",
      "Epoch 420/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.8712 - acc: 0.6850 - val_loss: 0.9838 - val_acc: 0.6750\n",
      "Epoch 421/500\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 0.8833 - acc: 0.6870 - val_loss: 0.9854 - val_acc: 0.6670\n",
      "Epoch 422/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 0.8481 - acc: 0.6950 - val_loss: 0.9810 - val_acc: 0.6680\n",
      "Epoch 423/500\n",
      "1000/1000 [==============================] - 1s 662us/step - loss: 0.8788 - acc: 0.7060 - val_loss: 0.9877 - val_acc: 0.6600\n",
      "Epoch 424/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 0.8748 - acc: 0.6760 - val_loss: 0.9811 - val_acc: 0.6630\n",
      "Epoch 425/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 0.8448 - acc: 0.6890 - val_loss: 0.9767 - val_acc: 0.6710\n",
      "Epoch 426/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 0.8441 - acc: 0.7000 - val_loss: 0.9780 - val_acc: 0.6680\n",
      "Epoch 427/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 0.8220 - acc: 0.7210 - val_loss: 0.9843 - val_acc: 0.6690\n",
      "Epoch 428/500\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 0.8484 - acc: 0.6970 - val_loss: 0.9802 - val_acc: 0.6710\n",
      "Epoch 429/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.8575 - acc: 0.6880 - val_loss: 0.9859 - val_acc: 0.6720\n",
      "Epoch 430/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 0.8759 - acc: 0.6860 - val_loss: 0.9805 - val_acc: 0.6690\n",
      "Epoch 431/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 0.8417 - acc: 0.7090 - val_loss: 0.9665 - val_acc: 0.6750\n",
      "Epoch 432/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 0.8560 - acc: 0.6930 - val_loss: 0.9772 - val_acc: 0.6690\n",
      "Epoch 433/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.8665 - acc: 0.6900 - val_loss: 0.9711 - val_acc: 0.6760\n",
      "Epoch 434/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 0.8852 - acc: 0.6940 - val_loss: 0.9811 - val_acc: 0.6690\n",
      "Epoch 435/500\n",
      "1000/1000 [==============================] - 1s 674us/step - loss: 0.8459 - acc: 0.6890 - val_loss: 0.9631 - val_acc: 0.6740\n",
      "Epoch 436/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 0.8667 - acc: 0.6920 - val_loss: 0.9733 - val_acc: 0.6690\n",
      "Epoch 437/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 0.8400 - acc: 0.7140 - val_loss: 0.9600 - val_acc: 0.6780\n",
      "Epoch 438/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 0.8432 - acc: 0.6990 - val_loss: 0.9656 - val_acc: 0.6760\n",
      "Epoch 439/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.8256 - acc: 0.6980 - val_loss: 0.9629 - val_acc: 0.6700\n",
      "Epoch 440/500\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 0.8241 - acc: 0.6930 - val_loss: 0.9622 - val_acc: 0.6760\n",
      "Epoch 441/500\n",
      "1000/1000 [==============================] - 1s 669us/step - loss: 0.8792 - acc: 0.6910 - val_loss: 0.9560 - val_acc: 0.6740\n",
      "Epoch 442/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.8551 - acc: 0.7040 - val_loss: 0.9746 - val_acc: 0.6690\n",
      "Epoch 443/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 0.8405 - acc: 0.7150 - val_loss: 0.9681 - val_acc: 0.6740\n",
      "Epoch 444/500\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 0.8037 - acc: 0.7040 - val_loss: 0.9662 - val_acc: 0.6720\n",
      "Epoch 445/500\n",
      "1000/1000 [==============================] - 1s 679us/step - loss: 0.8640 - acc: 0.6830 - val_loss: 0.9644 - val_acc: 0.6780\n",
      "Epoch 446/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.8750 - acc: 0.6950 - val_loss: 0.9768 - val_acc: 0.6690\n",
      "Epoch 447/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 0.8078 - acc: 0.7210 - val_loss: 0.9605 - val_acc: 0.6810\n",
      "Epoch 448/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.8108 - acc: 0.7180 - val_loss: 0.9607 - val_acc: 0.6790\n",
      "Epoch 449/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.8118 - acc: 0.7190 - val_loss: 0.9751 - val_acc: 0.6670\n",
      "Epoch 450/500\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 0.8379 - acc: 0.6930 - val_loss: 0.9567 - val_acc: 0.6760\n",
      "Epoch 451/500\n",
      "1000/1000 [==============================] - 1s 710us/step - loss: 0.8420 - acc: 0.6880 - val_loss: 0.9608 - val_acc: 0.6850\n",
      "Epoch 452/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 0.8445 - acc: 0.7050 - val_loss: 0.9657 - val_acc: 0.6660\n",
      "Epoch 453/500\n",
      "1000/1000 [==============================] - 1s 673us/step - loss: 0.8166 - acc: 0.7080 - val_loss: 0.9700 - val_acc: 0.6710\n",
      "Epoch 454/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 0.8736 - acc: 0.6910 - val_loss: 0.9653 - val_acc: 0.6730\n",
      "Epoch 455/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 0.8492 - acc: 0.6900 - val_loss: 0.9646 - val_acc: 0.6730\n",
      "Epoch 456/500\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 0.8238 - acc: 0.7050 - val_loss: 0.9821 - val_acc: 0.6760\n",
      "Epoch 457/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.8299 - acc: 0.7090 - val_loss: 0.9678 - val_acc: 0.6790\n",
      "Epoch 458/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 0.8149 - acc: 0.7150 - val_loss: 0.9823 - val_acc: 0.6730\n",
      "Epoch 459/500\n",
      "1000/1000 [==============================] - 1s 667us/step - loss: 0.8227 - acc: 0.7130 - val_loss: 0.9742 - val_acc: 0.6730\n",
      "Epoch 460/500\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 0.8283 - acc: 0.7100 - val_loss: 0.9560 - val_acc: 0.6770\n",
      "Epoch 461/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 0.8190 - acc: 0.6970 - val_loss: 0.9786 - val_acc: 0.6670\n",
      "Epoch 462/500\n",
      "1000/1000 [==============================] - 1s 673us/step - loss: 0.8070 - acc: 0.7040 - val_loss: 0.9741 - val_acc: 0.6740\n",
      "Epoch 463/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 0.8181 - acc: 0.7130 - val_loss: 0.9813 - val_acc: 0.6610\n",
      "Epoch 464/500\n",
      "1000/1000 [==============================] - 1s 677us/step - loss: 0.8030 - acc: 0.7120 - val_loss: 0.9787 - val_acc: 0.6710\n",
      "Epoch 465/500\n",
      "1000/1000 [==============================] - 1s 672us/step - loss: 0.8215 - acc: 0.7140 - val_loss: 0.9620 - val_acc: 0.6850\n",
      "Epoch 466/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.8084 - acc: 0.7050 - val_loss: 0.9764 - val_acc: 0.6700\n",
      "Epoch 467/500\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 0.8050 - acc: 0.7040 - val_loss: 0.9745 - val_acc: 0.6670\n",
      "Epoch 468/500\n",
      "1000/1000 [==============================] - 1s 665us/step - loss: 0.8332 - acc: 0.7030 - val_loss: 0.9620 - val_acc: 0.6690\n",
      "Epoch 469/500\n",
      "1000/1000 [==============================] - 1s 682us/step - loss: 0.8191 - acc: 0.6950 - val_loss: 0.9737 - val_acc: 0.6700\n",
      "Epoch 470/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 0.7926 - acc: 0.7120 - val_loss: 0.9683 - val_acc: 0.6730\n",
      "Epoch 471/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 0.8316 - acc: 0.6980 - val_loss: 0.9663 - val_acc: 0.6740\n",
      "Epoch 472/500\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 0.8245 - acc: 0.7210 - val_loss: 0.9557 - val_acc: 0.6790\n",
      "Epoch 473/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.7812 - acc: 0.7220 - val_loss: 0.9621 - val_acc: 0.6770\n",
      "Epoch 474/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 0.8353 - acc: 0.6990 - val_loss: 0.9662 - val_acc: 0.6760\n",
      "Epoch 475/500\n",
      "1000/1000 [==============================] - 1s 700us/step - loss: 0.8111 - acc: 0.7170 - val_loss: 0.9921 - val_acc: 0.6620\n",
      "Epoch 476/500\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 0.8079 - acc: 0.7260 - val_loss: 0.9796 - val_acc: 0.6700\n",
      "Epoch 477/500\n",
      "1000/1000 [==============================] - 1s 678us/step - loss: 0.8064 - acc: 0.7110 - val_loss: 0.9766 - val_acc: 0.6730\n",
      "Epoch 478/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 0.8023 - acc: 0.7060 - val_loss: 0.9735 - val_acc: 0.6810\n",
      "Epoch 479/500\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 0.7818 - acc: 0.7140 - val_loss: 0.9662 - val_acc: 0.6750\n",
      "Epoch 480/500\n",
      "1000/1000 [==============================] - 1s 661us/step - loss: 0.7733 - acc: 0.7160 - val_loss: 0.9720 - val_acc: 0.6720\n",
      "Epoch 481/500\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.7804 - acc: 0.7170 - val_loss: 0.9714 - val_acc: 0.6750\n",
      "Epoch 482/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.8339 - acc: 0.7150 - val_loss: 0.9638 - val_acc: 0.6790\n",
      "Epoch 483/500\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 0.8149 - acc: 0.7070 - val_loss: 0.9678 - val_acc: 0.6760\n",
      "Epoch 484/500\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.8088 - acc: 0.7120 - val_loss: 0.9662 - val_acc: 0.6780\n",
      "Epoch 485/500\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 0.8313 - acc: 0.7070 - val_loss: 0.9591 - val_acc: 0.6770\n",
      "Epoch 486/500\n",
      "1000/1000 [==============================] - 1s 661us/step - loss: 0.7771 - acc: 0.7260 - val_loss: 0.9663 - val_acc: 0.6800\n",
      "Epoch 487/500\n",
      "1000/1000 [==============================] - 1s 712us/step - loss: 0.8243 - acc: 0.7070 - val_loss: 0.9707 - val_acc: 0.6730\n",
      "Epoch 488/500\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 0.7890 - acc: 0.7080 - val_loss: 0.9615 - val_acc: 0.6810\n",
      "Epoch 489/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 0.7981 - acc: 0.7050 - val_loss: 0.9592 - val_acc: 0.6730\n",
      "Epoch 490/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 0.7991 - acc: 0.7070 - val_loss: 0.9610 - val_acc: 0.6810\n",
      "Epoch 491/500\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 0.7985 - acc: 0.7090 - val_loss: 0.9546 - val_acc: 0.6810\n",
      "Epoch 492/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 0.7681 - acc: 0.7250 - val_loss: 0.9698 - val_acc: 0.6790\n",
      "Epoch 493/500\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 0.7898 - acc: 0.7130 - val_loss: 0.9600 - val_acc: 0.6840\n",
      "Epoch 494/500\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 0.8208 - acc: 0.7170 - val_loss: 0.9574 - val_acc: 0.6770\n",
      "Epoch 495/500\n",
      "1000/1000 [==============================] - 1s 673us/step - loss: 0.8163 - acc: 0.7210 - val_loss: 0.9743 - val_acc: 0.6740\n",
      "Epoch 496/500\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 0.7740 - acc: 0.7320 - val_loss: 0.9657 - val_acc: 0.6800\n",
      "Epoch 497/500\n",
      "1000/1000 [==============================] - 1s 671us/step - loss: 0.7918 - acc: 0.7060 - val_loss: 0.9609 - val_acc: 0.6780\n",
      "Epoch 498/500\n",
      "1000/1000 [==============================] - 1s 665us/step - loss: 0.7538 - acc: 0.7250 - val_loss: 0.9538 - val_acc: 0.6800\n",
      "Epoch 499/500\n",
      "1000/1000 [==============================] - 1s 673us/step - loss: 0.7983 - acc: 0.6920 - val_loss: 0.9599 - val_acc: 0.6800\n",
      "Epoch 500/500\n",
      "1000/1000 [==============================] - 1s 675us/step - loss: 0.7816 - acc: 0.7160 - val_loss: 0.9442 - val_acc: 0.6860\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([story_train, queries_train], answers_train,batch_size=32,\n",
    "                    epochs=500,validation_data=([story_test, queries_test], \n",
    "                                                answers_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "yoN2AEOkkt4k",
    "outputId": "98da20c2-8be7-4754-bd03-fac0f2d7c95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1jYlpeiqsK1C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "End-to-End Memory Networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
